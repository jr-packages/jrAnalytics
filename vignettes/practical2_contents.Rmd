```{r, include=FALSE}
library(knitr)
library(jrNotes)
opts_chunk$set(self.contained = FALSE, tidy = TRUE,
              cache = TRUE, size = "small", message = FALSE,
              # fig.path=paste0('knitr_figure/', fname),
               # cache.path=paste0('knitr_cache/',fname),
               fig.align = "center",
               dev = "pdf", fig.width = 5, fig.height = 5)

knit_hooks$set(par = function(before, options, envir) {
    if (before && options$fig.show != "none") {
        par(mar = c(3, 3, 2, 1), cex.lab = .95, cex.axis = .9,
            mgp = c(2, .7, 0), tcl = -.01, las = 1)
}}, crop = hook_pdfcrop)


#opts_knit$set(out.format = "latex")
options(width = 56)
#dir.create("graphics",showWarnings = FALSE)
```

During this practical we will mainly use the **caret** package, we should load that package

```{r, echo = TRUE}
library("caret")
```

## The `cars2010` data set

The `cars2010` data set contains information about car models in $2010$. The aim is to model the `FE` variable which is a fuel economy measure based on $13$
predictors.^[Further information can be found in the help page,
`help("cars2010", package = "AppliedPredictiveModeling")`.]

The data is part of the **AppliedPredictiveModeling** package and can be loaded by

```{r, echo = TRUE}
data(FuelEconomy, package = "AppliedPredictiveModeling")
```

## Exploring the data


  1. Prior to any analysis we should get an idea of the relationships between variables in the data. ^[The `FE ~ .` notation is shorthand for `FE` against all variables in the data frame specified by the `data` argument.] Use the `pairs()` function to explore the data. The first few are shown in figure @\ref(fig:fig1_1).

  1. An alternative to using `pairs()` is to specify a plot device that has enough
space for the number of plots required to plot the response against
each predictor

    ```{r, echo = TRUE,fig.keep="none"}
    op = par(mfrow = c(3, 5), mar = c(4, 2, 1, 1.5))
    plot(FE ~ ., data = cars2010)
    par(op)
    ```


  We don't get all the pairwise information amongst predictors but it saves a lot of space on the plot and makes it easier to see what's going on. It is also a good idea to make smaller margins.


```{r fig1_1, fig.cap="Plotting the response against some of the predictor variables in the `cars2010` data set.", fig.width=9,fig.height=4.5, echo = FALSE, fig.pos="t"}
set_nice_par(mfrow = c(1, 2))
set_palette(1)
plot(FE ~ EngDispl + NumCyl, data = cars2010, pch = 21, bg = 1,
     ylim = c(0, 75), cex = 0.7)
```

  1. Create a simple linear model fit of `FE` against `EngDispl` using the `train()` function^[Hint: use the `train()` function with the `lm` method.]. Call your model `m1`. 
  
    ```{r}
    m1 = train(FE ~ EngDispl, method = "lm", data = cars2010)
    ```
  
 1. Examine the residuals of this fitted model, plotting residuals against fitted values

    ```{r, fig.keep="none"}
    rstd = rstandard(m1$finalModel)
    plot(fitted.values(m1$finalModel), rstd)
    ```

  1. We can add the lines showing where we expect the standardised residuals to fall to aid graphical inspection

    ```{r, echo = TRUE,eval=FALSE}
    abline(h = c(-2, 0, 2), col = 2:3, lty = 2:1)
    ```


  1. What do the residuals tell us about the model fit using this plot?
  
    ```{r, tidy=TRUE}
    # There definitely appears to be some trend in the residuals.
    # The curved shape indicates that we potentially require some
    # transformation of variables. A squared term might help.
    ```
  

```{r, fig.margin=TRUE, fig.cap="Plot of fitted against observed values. It's always important to pay attention to the scales."}
set_nice_par()
set_palette(1)
plot(cars2010$FE, fitted.values(m1$finalModel),
     xlab = "FE", ylab = "Fitted values",
     xlim = c(10, 75), ylim = c(10, 75))
abline(0, 1, col = 3, lty = 2)
```

  1. Plot the fitted values vs the observed values

    ```{r, fig.keep='none'}
    plot(cars2010$FE, fitted.values(m1$finalModel),
         xlab = "FE", ylab = "Fitted values",
         xlim = c(10, 75), ylim = c(10, 75))
    ```

  1. What does this plot tell us about the predictive performance of this model across the range of the response?

    ```{r}
    # We seem to slightly over estimate more often than not in the 25-35
    # range. For the upper end of the range we seem to always under
    # estimate the true values.
    ```


  1. Produce other diagnostic plots of this fitted model, e.g. a q-q plot

    ```{r, fig.keep='none'}
    qqnorm(rstd); qqline(rstd)
    plot(cars2010$EngDispl, rstd)
    abline(h = c(-2, 0, 2), col =  2:3, lty = 1:2)
    ```


  1. Are the modelling assumptions justified?

    ```{r}
    # We are struggling to justify the assumption of normality in the
    # residuals here, all of the diagnostics indicate patterns remain in
    # the residuals that are currently unexplained by the model.    
    ```


<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

## Extending the model

  1. Do you think adding a quadratic term will improve the model fit?

    ```{r, tidy = TRUE}
    # We are struggling to justify the assumption of normality in the
    # residuals here, all of the diagnostics indicate patterns remain in
    # the residuals that are currently unexplained by the model 
    # so potentially a parabola will help 
    ```

  1. Fit a model with the linear and quadratic terms for `EngDispl` and call it `m2`

    ```{r}
    m2 = train(FE ~ poly(EngDispl, 2, raw = TRUE), data = cars2010,
        method = "lm")
    ```

  1. Assess the modelling assumptions for this new model. How do the two models compare?

    ```{r, tidy=TRUE}
    # The residual diagnostics indicate a better fit now that the quadratic
    # term has been included.
    ```


  1. Add `NumCyl` as a predictor to the simple linear regression model `m1` and call it `m3`

    ```{r}
    m3 = train(FE~EngDispl + NumCyl, data = cars2010, method = "lm")
    ```


  1. Examine model fit and compare to the original.
  
  1. Does the model improve with the addition of an extra variable?

## Visualising the model

The **jrAnalytics** package contains a `plot3d()` function to help with viewing these surfaces in 3D.^[We can also add the observed points to the plot using the `points()` argument to this function, see the help page for further information.]

```{r,echo=TRUE,fig.keep="none"}
## points = TRUE to also show the points
library(jrAnalytics)
plot3d(m3, cars2010$EngDispl, cars2010$NumCyl,
       cars2010$FE, points = FALSE)
```

We can also examine just the data interactively, via

```{r, eval = FALSE, echo = TRUE}
threejs::scatterplot3js(cars2010$EngDispl, cars2010$NumCyl,
                        cars2010$FE, size = 0.5)
```

  1. Try fitting other variations of this model using these two predictors. For example, try adding polynomial and interaction terms

    ```{r}
    m4 = train(FE~EngDispl * NumCyl + I(NumCyl^5),
               data = cars2010, method = "lm")
    ```

  1. How is prediction affected in each case? Don't forget to examine residuals, R squared values and the predictive surface.
  1. If you want to add an interaction term you can do so with the `:` operator, how does the interaction affect the surface?


## Other data sets

A couple of other data sets that can be used to try fitting linear regression models.


Data set      | Package     | Response 
--------------|-------------|---------  
diamonds      | **ggplot2** | price 
Wage          | **ISLR**    | wage
BostonHousing | **mlbench** | medv 
--------------|-------------|----------
